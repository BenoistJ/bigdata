####
val logFile = "hdfs:///user/root/prepared/big/data.csv"

val rdd1 = sc.textFile(logFile).
    map(line => line.split(";").
    map(_.trim))

val rdd2 = rdd1.
  filter(alr => alr(1) == "www.mydemo.com").
  map(alr => (alr(3), alr(0))).
  distinct().
  map{ case ((monthDate, _)) => (monthDate, 1) }.
  reduceByKey(_ + _).
  sortBy(_._1, ascending=true)
  
rdd2.collect().foreach(println)


####
val logFile = "hdfs:///user/root/prepared/big/data.csv"

case class AccessLogRecord (
    clientIpAddress: String,
    domainName: String,
    remoteUser: String,
    monthDate: String,
    request: String,
    httpStatusCode: String,
    bytesSent: String,
    referer: String,
    userAgent: String
)

val df1 = sc.textFile(logFile).
    map(line => line.split(";").
    map(_.trim)).
    map(t => AccessLogRecord(t(0), t(1), t(2), t(3), t(4), t(5), t(6), t(7), t(8))).
    toDF
df1.registerTempTable("df1")

val df2 = sqlContext.sql(
      "SELECT DISTINCT monthDate, clientIpAddress " +
      "FROM df1 " +
      "WHERE domainName = 'www.mydemo.com'")
df2.registerTempTable("df2")

val df3 = sqlContext.sql(
      "SELECT monthDate, count(clientIpAddress) as Total " +
      "FROM df2 " +
      "GROUP BY monthDate " +
      "ORDER BY monthDate ASC " +
      "LIMIT 10")
df3.show(false)


####
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn --executor-cores 3 examples/jars/spark-examples*.jar 10
